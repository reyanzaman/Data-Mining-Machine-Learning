# -*- coding: utf-8 -*-
"""CSE417_Assignment-4_2021065.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UiW8V8mYgAE6tAijLtXpNvpyuT8Mu-Sq
"""

from google.colab import drive
drive.mount('/content/drive')

#@title Import Statements

# import bs4
# import requests
# import argparse
# import sys
# import os
# import random
# import math
# import json
# import sys
# import time

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# import os
# import json
# from datetime import date, timedelta
# from bs4 import BeautifulSoup
# import requests

import requests
from bs4 import BeautifulSoup
import time
import csv

#@title Importing TSV File
dataset = pd.read_csv('/content/drive/MyDrive/Data Mining/train.tsv', sep='\t')

#Selecting random labels
selected_data = pd.DataFrame()
unique_labels = np.unique(dataset['class_label']) #There is apparently only 6 labels
#random_labels = np.random.permutation(unique_labels)

#Selecting 200 labels from the random labels
for label in unique_labels:
    label_data = dataset[dataset['class_label'] == label].sample(200)
    selected_data = pd.concat([selected_data, label_data])

#Converting
data = selected_data['text'].to_list()
labels = selected_data['class_label'].to_numpy()

#Print the shape of the selected data and labels
print(len(data))
print(labels.shape)
print(np.unique(labels))

import nltk
import re
nltk.download('punkt')

#@title Bangla Tokenizer

#Fixing the text for dari and dash
processed_text = []
for lines in data:
    text = re.sub(r'।', ' । ', lines)
    text = re.sub(r'-', ' - ', text)
    text = re.sub(r'—', ' — ', text)
    text = re.sub(r'\.', ' . ', text)
    processed_text.append(text)

#print(len(processed_text))

#2D Array. Different Texts in Rows. Tokenized Words of that text in columns
tokenized_words = []
for text in processed_text:
  tokenized_words.append(nltk.word_tokenize(text))

#Printing the first 15 tokenized words of 6 texts from different categories
for j in range(0, 1200, 200):
  print("Label: " + labels[j])
  for i in range(15):
    print("\'" + tokenized_words[j][i] + "\'", end=" ")
  print()

#Does not work. Causes index out of range error
# !pip install py-bangla-stemmer

#https://pypi.org/project/bangla-stemmer/
!pip install bangla-stemmer

#@title Bangla Stemming

# from bangla_stemmer.stemmer import stemmer
# wordlist = ['কবিরগুলিকে', 'আমাকে', 'নামাবার']
# stmr = stemmer.BanglaStemmer()
# stm = stmr.stem(word)
# print(stm)

from bangla_stemmer.stemmer import stemmer
stmr = stemmer.BanglaStemmer()

stemmed_words = []
for lines in tokenized_words:
  temp_list = []
  for words in lines:
    temp_list.append(stmr.stem(words))
  stemmed_words.append(temp_list)

#@title Viewing Stemmed Words

print(len(stemmed_words))

#Printing the first 15 stemmed words of 6 texts from different categories
for j in range(0, 1200, 200):
  print("Label: " + labels[j])
  for i in range(15):
    print("\'" + stemmed_words[j][i] + "\'", end=" ")
  print()

import string
nltk.download('stopwords')
from nltk.corpus import stopwords

print(list(stopwords.words()))

#@title Normalizing Stemmed Words

#Removing the stop words
stop_words = list(stopwords.words())
normalized_list = []
for line in stemmed_words:
    normalized_line = []
    for words in line:
        if words not in stop_words:
            #The document also has some english words
            normalized_words = words.lower()
            normalized_line.append(normalized_words)
    normalized_list.append(normalized_line)

#Removing all the punctuation marks

normalized = []
punctuation = string.punctuation
punctuation = punctuation + "‘’।-''—"
for lines in normalized_list:
  temp_list = []
  for words in lines:
    if words not in punctuation:
      temp_list.append(words)
  normalized.append(temp_list)

# Removing all the digits as the document has some digits in it
normalized_words_list = []
for line in normalized:
    words_list = []
    for words in line:
        if not words.isdigit():
            words_list.append(words)
    normalized_words_list.append(words_list)

#@title Printing Normalizing Stemmed Words

print("Tokenized words")
for i in range(5):
  for j in range(15):
    print(tokenized_words[i][j], end=" ")
  print()

print()
print("Stemmed words")
for i in range(5):
  for j in range(15):
    print(stemmed_words[i][j], end=" ")
  print()

print()
print("Normalized words")
for i in range(5):
  for j in range(15):
    print(normalized_list[i][j], end=" ")
  print()

print()
print("No digits or punctuation normalized words")
for i in range(5):
  for j in range(15):
    print(normalized_words_list[i][j], end=" ")
  print()

#@title Zipf's Law

from collections import Counter

# Flatten the 2D list into a 1D list of words
flattened_words = []
for lines in normalized_words_list:
  for words in lines:
    flattened_words.append(words)

# Count the frequency of each word in the flattened list
word_count = Counter(flattened_words)
word_count = dict(sorted(word_count.items(), key=lambda item: item[1], reverse=True))

#Deleting 5% of most and least frequent terms
#Data is skewed. Is there a way to analyze this and set the percentage accordingly?
num_words = len(word_count)
lower_cutoff = int(num_words * 0.01) #less unnecessary data with more frequency
upper_cutoff = int(num_words * 0.1) #lots of unnecessary data with least frequency

most_frequent_words = list(word_count.keys())[:lower_cutoff]
least_frequent_words = list(word_count.keys())[-upper_cutoff:]
delete_words = most_frequent_words + least_frequent_words

term_list = []
for lines in normalized_words_list:
    temp_list = []
    for words in lines:
        if words not in delete_words:
            temp_list.append(words)
    term_list.append(temp_list)

print(num_words)
print("Word Count: ", word_count)
print(len(delete_words))
print("Delete Words: ", delete_words)

flattened_words2 = []
for lines in term_list:
  for words in lines:
    flattened_words2.append(words)
word_count2 = Counter(flattened_words2)
print(len(word_count2))
# print("Term List: ", term_list)
for i in range(100):
  print(term_list[i])

print("Before applying zipf's law")
print()
for i in range(5):
  for j in range(15):
    print(normalized_words_list[i][j], end=" ")
  print()

print()
print("Zipf's Law Applied")
print()
for i in range(5):
  for j in range(15):
    print(term_list[i][j], end=" ")
  print()

#@title Term Frequency Matrix

#sets are hashable, using a list gives error
all_terms = set()
for terms in term_list:
    all_terms.update(terms)

#creating an empty matrix with terms in rows and documents in columns
tf_matrix = {}
for terms in all_terms:
    tf_matrix[terms] = [0] * len(term_list)

#incrementing count of terms
for i, doc_terms in enumerate(term_list):
    for terms in doc_terms:
        tf_matrix[terms][i] += 1

#printing
print(len(tf_matrix))
for term, freqs in tf_matrix.items():
    print(term, freqs)

#@title Normalized Term Frequency Matrix

tf_norm_matrix = {}

#Frequencies divided by the total number of terms in the document
for term, freqs in tf_matrix.items():
    term_sum = sum(freqs)
    norm_freqs = []
    for freq in freqs:
        norm = freq / term_sum
        norm_freqs.append(norm)
    tf_norm_matrix[term] = norm_freqs

# print the normalized term frequency matrix
for term, freqs in tf_norm_matrix.items():
    print(term, freqs)

#@title Inverse Document Frequency

import math

idf = {}
num_docs = len(term_list)
print("Number of documents: ", num_docs)

for term, freqs in tf_norm_matrix.items():
    # count the number of documents that contains the current term
    doc_term_count = sum([freq > 0 for freq in freqs])
    #IDF(T) = 1 + log( (N) / (df(T)) )
    idf_T = 1 + math.log(num_docs / doc_term_count)
    idf[term] = idf_T

#Sorting according to IDF values
idf_sorted = sorted(idf.items(), key=lambda x: x[1], reverse=True)

#Printing
for term, idf in idf_sorted:
    print(term, idf)

#@title Final Weighted Matrix

weighted_matrix = {}

for term, freqs in tf_norm_matrix.items():
    weighted_values = []
    for idf_term, idf_value in idf_sorted:
        if idf_term == term:
            for freq in freqs:
                weighted_values.append(freq * idf_value)
            break
    weighted_matrix[term] = weighted_values

# print the final weighted matrix for all documents
for term, weights in weighted_matrix.items():
    print(term, weights)

#@title Simple centroid initialization function
def initialize_centroids_simple(data, dimension, k):
    #centroids: [[centroid0:  3 dimensions, , , ]; [centroid1: 3 dimensions ] ... ..]
    centroids = np.array([[0 for _ in range(dimension)] for _ in range(k)])
    #TO DO
    #Write your code to return initialized centroids by randomly assiging them to K points
    centroids_indices = np.random.choice(data.shape[0], size=k, replace=False)
    centroids = data[centroids_indices, :]
    return centroids

#@title Calculating Euclidean Distance
def get_euclidean_distance(p1, p2):
    #TODO
    #Write your code
    distance = np.sqrt(np.sum((p1 - p2) ** 2))
    return distance

#@title KMeans Function

def kmeans(data, dimension, k):
    N = np.size(data, 0)
    #centroids: [[centroid0:  , , ,3 dimensions, , , ],  [centroid1: , , ,3 dimensions, , , ],  ... ..]
    centroids = initialize_centroids_simple(data, dimension, k)
    #cluster_affiliation: cluster_affiliation = [clusternumber, clutsernumber, ..., ..., ..., ...]

    #initialize the cluster affiliations. Initially assign -1
    cluster_affiliation = np.array([-1 for _ in range(0, N)])
    flag = 1
    Jprev = 0
    while flag:
        #find closest centroids for each data points
        distances = []
        min_distance = float('inf')
        for point in data:
            temp = []
            for centroid in centroids:
                distance = get_euclidean_distance(centroid, point)
                temp.append(distance)
            distances.append(temp)
        #record or update cluster for each data points
        cluster_affiliation = np.argmin(distances, axis=1)
        #recompute centroids
        centroids = np.array([[0 for _ in range(dimension)] for _ in range(k)]) # new centroids initialized with 0
        clutser_point_count = np.array([0 for _ in range(k)]) #keep number of points for each cluster. You should use
        #cluster_affiliation  to calculate it

        #TO DO
        #write your code to count each cluster pointcount and store them in cluster_point_count structure
        cluster_point_count = np.bincount(cluster_affiliation, minlength=k)

        #TODO
        #recompute centroids using the count
        centroids = np.zeros((k, dimension))
        for i in range(k):
            if cluster_point_count[i] > 0:
                centroids[i] = np.mean(data[cluster_affiliation == i], axis=0)

        #TODO
        #write your own code  to terminate the process based on the termination criteria. We evaluate the quality
        #of the clustering using the clustering objective discussed in the class.
        #we terminate when |J − Jprev| ≤ math.power(10,−5)*J, where Jprev is the value of J after the previous iteration).
        #Set flag = 0 if |J − Jprev| ≤ math.power(10,−5)*J
        J = np.sum(np.min(distances, axis=1))/np.size(data,0)
        if abs(J - Jprev) <= 1e-5 * J:
            flag = 0
        Jprev = J
    return (centroids, cluster_affiliation)

#converting dictionary into 2D numpy array
weighted_array = np.array(list(weighted_matrix.values()))
transposed_array = np.transpose(weighted_array)

print(transposed_array)
print(np.size(transposed_array, 1))

#@title K-Means clustering on documents

K = 6 # K clusters

doc_data = transposed_array

dimension = np.size(doc_data, 1)
centroids, cluster_affiliation = kmeans(doc_data, dimension, K)

#1200 documents 23535 terms
print("Data shape: ", doc_data.shape)
print("Centroids: ", centroids, sep="\n")
print("Centroid shape", centroids.shape)
print("Cluster Affiliation: ", cluster_affiliation, sep="\n")
print("Cluster Affiliation shape", cluster_affiliation.shape)
print(Counter(cluster_affiliation))

#@title Davies-Bouldin Index

#DB = (1/k) * ∑i=1 to k(maxj≠i(Ri+Rj) / d(ci,cj))

db_index = 0
for i in range(K):
    # Calculating Ri & Ci
    ci = centroids[i]
    data_i = doc_data[cluster_affiliation == i]
    ri = np.mean(get_euclidean_distance(data_i, ci)) #Intra-Cluster-Similarity

    max_similarity = 0
    for j in range(K):
        if i != j:
            # Calculating Rj & Cj
            cj = centroids[j]
            data_j = doc_data[cluster_affiliation == j]
            rj = np.mean(get_euclidean_distance(data_j, cj))

            # Calculate d(ci, cj)
            dist_centroids = get_euclidean_distance(ci, cj)

            similarity = (ri + rj) / dist_centroids

            if similarity > max_similarity:
                max_similarity = similarity

    # Update the Davies-Bouldin Index
    db_index += max_similarity

db_index = db_index * 1/K

print("Davies-Bouldin Index:", db_index)

#@title Dunn Index
#DI = Min_InterCluster_Distance / Max_IntraCluster_Distance

min_intercluster_distance = np.inf
max_intracluster_distance = np.NINF

for i in range(K):
    for j in range(K):
        if i != j:
            #Distances between data points (Intra-Cluster Distance)
            data_i = doc_data[cluster_affiliation == i]
            data_j = doc_data[cluster_affiliation == j]
            distances = []

            for x in data_i:
                for y in data_j:
                    distance = get_euclidean_distance(x, y)
                    distances.append(distance)

            intracluster_distance = np.max(distances)
            if intracluster_distance > max_intracluster_distance:
                max_intracluster_distance = intracluster_distance

            #Distances between centroids (Inter-Cluster Distance)
            ci = centroids[i]
            cj = centroids[j]
            dist = get_euclidean_distance(ci, cj)

            if dist < min_intercluster_distance:
                min_intercluster_distance = dist

dunn_index = min_intercluster_distance / max_intracluster_distance
print("Dunn Index:", dunn_index)

#@title Purity

#Purity = (1/n) * Σi maxj |Ci ∩ Gj|

#print("Labels shape: ", labels.shape)
num_docs = labels.shape[0]

#Converting string labels into integer using dictionary
label_dict = {}
for i, label in enumerate(unique_labels):
    label_dict[label] = i

int_labels = np.zeros(labels.shape[0], dtype=int)

for i, label in enumerate(labels):
    int_labels[i] = label_dict[label]

#print("Int_Labels: ", int_labels)

#Finding Purity
total_correct = 0

for i in range(K):
    cluster_int_labels = int_labels[cluster_affiliation == i]
    most_common_label = np.bincount(cluster_int_labels).argmax()
    num_correct = np.sum(cluster_int_labels == most_common_label)
    total_correct += num_correct

purity = total_correct / num_docs
print("Purity:", purity)

#@title RAND Index

#RI = (TP + TN) / (TP + TN + FP + FN)

# TP is the number of pairs of points that are in the same cluster in both the predicted and actual clusterings
# TN is the number of pairs of points that are in different clusters in both the predicted and actual clusterings
# FP is the number of pairs of points that are in the same cluster in the actual clustering but in different clusters in the predicted clustering
# FN is the number of pairs of points that are in different clusters in the actual clustering but in the same cluster in the predicted clustering

TP = 0
TN = 0
FP = 0
FN = 0

for i in range(num_docs):
  for j in range(i+1, num_docs):
    #Same Cluster
    if cluster_affiliation[i] == cluster_affiliation[j]:
      #Same Cluster Same Labels
      if labels[i] == labels[j]:
          TP += 1
      #Same Cluster Different Labels
      else:
          FP += 1
    #Different Cluster
    else:
      #Different Cluster Same Labels
      if labels[i] == labels[j]:
          FN += 1
      #Different Cluster Different Labels
      else:
          TN += 1

RI = (TP + TN) / (TP + TN + FP + FN)

print("RAND Index: ", RI)

#@title TSNE 3D Conversion

from sklearn.manifold import TSNE

# Generate 3D data using TSNE
tsne = TSNE(n_components=2, perplexity=5, random_state=42)
doc_data_3d = tsne.fit_transform(doc_data)
centroid_3d = tsne.fit_transform(centroids)

print(doc_data_3d.shape)
print(centroid_3d.shape)
print(cluster_affiliation.shape)

#@title Plotting the 3D Data

fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111)

#Plotting the data points & centroids
colors = ['red', 'blue', 'green', 'purple', 'orange', 'black']
for i in range(K):
  ax.scatter(doc_data_3d[cluster_affiliation==i, 0], doc_data_3d[cluster_affiliation==i, 1], c=colors[i], alpha=0.5)
  ax.scatter(centroid_3d[:, 0][i], centroid_3d[:, 1][i], c=colors[i], s=200, marker='x')


plt.show()